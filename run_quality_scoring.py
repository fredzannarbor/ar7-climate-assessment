#!/usr/bin/env python3
"""
run_quality_scoring.py

Perform detailed quality scoring on AR7 generated chapters.
Evaluates accuracy, style, intelligence, and IPCC compliance.
"""

import argparse
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List

try:
    import litellm
    LITELLM_AVAILABLE = True
except ImportError:
    LITELLM_AVAILABLE = False
    print("WARNING: litellm not available. Quality scoring will be simulated.")


QUALITY_SCORING_PROMPT = """You are an expert IPCC reviewer with 20+ years of experience evaluating climate assessment reports.

Evaluate the following chapter on multiple quality dimensions using strict 1-7 Likert scales.

CHAPTER: {chapter_name}
MODEL: {model_name}
WORD COUNT: {word_count}

CONTENT:
{content}

RATING SCALES (1-7):

**ACCURACY** (Scientific correctness):
1 = Numerous errors, unreliable
3 = Several errors, mostly accurate on major points
5 = Minor errors only, highly accurate
7 = Flawless, every claim verifiable

**IPCC STYLE** (Format and conventions):
1 = Does not follow IPCC conventions
3 = Partial adherence, formatting issues
5 = Good IPCC style, minor deviations
7 = Perfect IPCC style, calibrated language, structure

**INTELLIGENCE** (Depth and sophistication):
1 = Superficial, misses key concepts
3 = Basic competence, adequate coverage
5 = National-level expert quality
7 = Superhuman insights beyond typical expertise

**COMPREHENSIVENESS** (Coverage completeness):
1 = Major gaps, incomplete
3 = Adequate but missing important aspects
5 = Thorough coverage of all key topics
7 = Exhaustive, covers everything expertly

**UNCERTAINTY LANGUAGE** (IPCC calibrated terms):
1 = Incorrect or missing uncertainty statements
3 = Some proper usage, some errors
5 = Good use of calibrated language
7 = Perfect application of IPCC terminology

**CITATION QUALITY** (if applicable):
1 = No references or poor quality
3 = Some appropriate references
5 = Well-referenced, appropriate sources
7 = Exemplary referencing and attribution

**SYNTHESIS QUALITY** (Integration of literature):
1 = No synthesis, disconnected facts
3 = Basic synthesis, some integration
5 = Good synthesis across literature
7 = Exceptional synthesis and integration

Provide scores and brief justifications in JSON:
{{
  "accuracy": <1-7>,
  "accuracy_justification": "...",
  "ipcc_style": <1-7>,
  "ipcc_style_justification": "...",
  "intelligence": <1-7>,
  "intelligence_justification": "...",
  "comprehensiveness": <1-7>,
  "comprehensiveness_justification": "...",
  "uncertainty_language": <1-7>,
  "uncertainty_language_justification": "...",
  "citation_quality": <1-7>,
  "citation_quality_justification": "...",
  "synthesis_quality": <1-7>,
  "synthesis_quality_justification": "...",
  "overall_score": <average of all scores>,
  "strengths": ["...", "...", "..."],
  "weaknesses": ["...", "...", "..."],
  "overall_assessment": "..."
}}
"""


def score_chapter(chapter_file: Path, model_name: str,
                 evaluator_model: str = "gemini/gemini-2.5-pro") -> Dict:
    """Score a single chapter"""

    print(f"  üìä Scoring: {chapter_file.name}")

    # Read chapter
    try:
        content = chapter_file.read_text(encoding='utf-8')
        word_count = len(content.split())

        # Truncate for evaluation (first 3000 words)
        if word_count > 3000:
            words = content.split()[:3000]
            content = ' '.join(words) + "\n\n[Content truncated for evaluation...]"
            print(f"    ‚ö†Ô∏è  Truncated to 3000 words for evaluation")

    except Exception as e:
        print(f"    ‚ùå Error reading file: {e}")
        return {"success": False, "error": str(e)}

    chapter_name = chapter_file.stem.replace('_', ' ').title()

    if not LITELLM_AVAILABLE:
        # Simulated scores
        import random
        return {
            "success": True,
            "chapter": chapter_name,
            "model": model_name,
            "word_count": word_count,
            "simulated": True,
            "accuracy": random.randint(5, 7),
            "ipcc_style": random.randint(5, 7),
            "intelligence": random.randint(4, 6),
            "comprehensiveness": random.randint(5, 7),
            "uncertainty_language": random.randint(5, 6),
            "citation_quality": random.randint(4, 6),
            "synthesis_quality": random.randint(4, 6),
            "overall_score": random.uniform(5.0, 6.5),
            "strengths": ["Simulated evaluation"],
            "weaknesses": ["litellm not available"],
            "overall_assessment": "Quality scoring simulated"
        }

    # Perform actual scoring
    try:
        prompt = QUALITY_SCORING_PROMPT.format(
            chapter_name=chapter_name,
            model_name=model_name,
            word_count=word_count,
            content=content
        )

        response = litellm.completion(
            model=evaluator_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)

        print(f"    ‚úÖ Overall score: {result.get('overall_score', 0):.1f}/7.0")

        return {
            "success": True,
            "chapter": chapter_name,
            "model": model_name,
            "word_count": word_count,
            "simulated": False,
            **result
        }

    except Exception as e:
        print(f"    ‚ùå Error during scoring: {e}")
        return {
            "success": False,
            "chapter": chapter_name,
            "model": model_name,
            "error": str(e)
        }


def main():
    parser = argparse.ArgumentParser(description="Quality scoring for AR7 chapters")
    parser.add_argument("--output-dir", default="output/ar7_complete_run_final")
    parser.add_argument("--samples", type=int, default=5,
                       help="Number of sample chapters to score per model")
    parser.add_argument("--evaluator", default="gemini/gemini-2.5-pro",
                       help="Model to use for scoring")
    parser.add_argument("--chapters",
                       help="Specific chapters to score (comma-separated)")

    args = parser.parse_args()

    output_dir = Path(args.output_dir)

    if not output_dir.exists():
        print(f"ERROR: Output directory not found: {output_dir}")
        return 1

    print(f"\n{'='*80}")
    print(f"AR7 QUALITY SCORING - SAMPLE CHAPTERS")
    print(f"{'='*80}")
    print(f"Evaluator Model: {args.evaluator}")
    print(f"Samples per Model: {args.samples}")
    print(f"{'='*80}\n")

    # Select chapters
    if args.chapters:
        target_chapters = [c.strip() for c in args.chapters.split(',')]
    else:
        target_chapters = [
            "summary_for_policymakers",
            "technical_summary",
            "chapter_2_vulnerabilities_impacts_risks",
            "chapter_7_africa",
            "chapter_14_terrestrial_ecosystems",
            "chapter_16_water",
            "chapter_19_health_wellbeing"
        ][:args.samples]

    print(f"Target chapters: {', '.join(target_chapters)}\n")

    # Score each model
    all_results = {}

    for model_dir in output_dir.iterdir():
        if not model_dir.is_dir() or model_dir.name in ['pdfs', '__pycache__', 'fact_checking', 'quality_scoring']:
            continue

        model_name = model_dir.name
        print(f"\n{'='*80}")
        print(f"MODEL: {model_name}")
        print(f"{'='*80}")

        model_results = []

        for chapter_key in target_chapters:
            chapter_file = model_dir / f"{chapter_key}.txt"

            if not chapter_file.exists():
                print(f"  ‚ö†Ô∏è  Skipping {chapter_key} - file not found")
                continue

            result = score_chapter(chapter_file, model_name, args.evaluator)
            model_results.append(result)

        all_results[model_name] = model_results

        # Summary for this model
        successful = [r for r in model_results if r.get("success")]
        if successful:
            avg_overall = sum(r.get("overall_score", 0) for r in successful) / len(successful)
            avg_accuracy = sum(r.get("accuracy", 0) for r in successful) / len(successful)
            avg_style = sum(r.get("ipcc_style", 0) for r in successful) / len(successful)

            print(f"\n  Summary for {model_name}:")
            print(f"    Chapters scored: {len(successful)}")
            print(f"    Average overall: {avg_overall:.2f}/7.0")
            print(f"    Average accuracy: {avg_accuracy:.2f}/7.0")
            print(f"    Average IPCC style: {avg_style:.2f}/7.0")

    # Save results
    quality_dir = output_dir / "quality_scoring"
    quality_dir.mkdir(exist_ok=True)

    results_file = quality_dir / "quality_scores.json"

    full_results = {
        "generated_at": datetime.now().isoformat(),
        "evaluator_model": args.evaluator,
        "chapters_scored": target_chapters,
        "models": all_results
    }

    with open(results_file, 'w') as f:
        json.dump(full_results, f, indent=2)

    print(f"\n{'='*80}")
    print(f"QUALITY SCORING COMPLETE")
    print(f"{'='*80}")
    print(f"Results saved to: {results_file}")

    # Generate report
    report_file = quality_dir / "quality_report.md"

    report = f"""# AR7 Quality Scoring Report

**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Evaluator Model**: {args.evaluator}
**Chapters Scored**: {len(target_chapters)}

## Scoring Scale (1-7 Likert)

- **7**: Exceptional, world-class quality
- **6**: Excellent, very high quality
- **5**: Good, professional quality
- **4**: Adequate, meets basic standards
- **3**: Below average, needs improvement
- **2**: Poor, significant issues
- **1**: Unacceptable, major problems

## Summary by Model

"""

    for model_name, results in sorted(all_results.items()):
        successful = [r for r in results if r.get("success")]

        if not successful:
            continue

        report += f"### {model_name}\n\n"

        # Calculate averages
        metrics = {
            "Overall Score": "overall_score",
            "Accuracy": "accuracy",
            "IPCC Style": "ipcc_style",
            "Intelligence": "intelligence",
            "Comprehensiveness": "comprehensiveness",
            "Uncertainty Language": "uncertainty_language",
            "Citation Quality": "citation_quality",
            "Synthesis Quality": "synthesis_quality"
        }

        report += "| Metric | Average | Rating |\n"
        report += "|--------|---------|--------|\n"

        for metric_name, metric_key in metrics.items():
            avg = sum(r.get(metric_key, 0) for r in successful) / len(successful)

            if avg >= 6:
                rating = "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent"
            elif avg >= 5:
                rating = "‚≠ê‚≠ê‚≠ê‚≠ê Good"
            elif avg >= 4:
                rating = "‚≠ê‚≠ê‚≠ê Adequate"
            else:
                rating = "‚≠ê‚≠ê Needs Improvement"

            report += f"| {metric_name} | {avg:.2f}/7.0 | {rating} |\n"

        report += "\n"

        # Strengths and weaknesses
        all_strengths = []
        all_weaknesses = []

        for r in successful:
            all_strengths.extend(r.get("strengths", []))
            all_weaknesses.extend(r.get("weaknesses", []))

        if all_strengths:
            report += "**Common Strengths:**\n"
            for strength in set(all_strengths[:5]):
                report += f"- {strength}\n"
            report += "\n"

        if all_weaknesses:
            report += "**Common Weaknesses:**\n"
            for weakness in set(all_weaknesses[:5]):
                report += f"- {weakness}\n"
            report += "\n"

    report += f"\n## Model Comparison\n\n"
    report += "| Model | Overall | Accuracy | Style | Intelligence |\n"
    report += "|-------|---------|----------|-------|-------------|\n"

    for model_name, results in sorted(all_results.items()):
        successful = [r for r in results if r.get("success")]
        if successful:
            overall = sum(r.get("overall_score", 0) for r in successful) / len(successful)
            accuracy = sum(r.get("accuracy", 0) for r in successful) / len(successful)
            style = sum(r.get("ipcc_style", 0) for r in successful) / len(successful)
            intelligence = sum(r.get("intelligence", 0) for r in successful) / len(successful)

            report += f"| {model_name} | {overall:.2f} | {accuracy:.2f} | {style:.2f} | {intelligence:.2f} |\n"

    report += f"\n## Detailed Results\n\nSee `quality_scores.json` for complete details and justifications.\n"

    report_file.write_text(report)
    print(f"Report saved to: {report_file}")
    print(f"{'='*80}\n")

    return 0


if __name__ == "__main__":
    sys.exit(main())

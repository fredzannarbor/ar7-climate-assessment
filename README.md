# AR7 Multi-Model Climate Assessment Comparison

A comprehensive framework for generating and evaluating draft climate assessment reports using multiple Large Language Models (LLMs), demonstrating transparent experimentation with AI involvement in scientific communication.

## Overview

This script generates complete drafts of the IPCC AR7 Working Group II climate assessment report (29 chapters, 330K+ words) using seven different AI models, then carries out a fact-checking loop, quality scoring, and multi-model comparison.

**Key Achievement**: Provides a framework for evaluating potential LLM contributions to climate assessment reviews.

**Not in Scope**: 

At present:

- This project does not aim to replace human expertise in climate science.
- This project does not carry out direct analysis of data or models.
- This project does not create images, tables, charts, or visualizations.
- No user acceptance testing has been conducted.
- All generated content requires expert validation.

---

## üåç Benefits to Global Publics, Science, and Policy

### For Global Publics

**1. Democratization of Scientific Knowledge**
- Makes complex climate science more accessible through AI-assisted synthesis
- Enables rapid generation of assessment summaries in multiple languages
- Reduces time from research to public-facing reports from years to hours

**2. Transparency and Trust**
- **Open evaluation frameworks** show exactly how AI content is generated and validated
- **Multi-model comparison** reveals strengths and weaknesses of different approaches
- **Fact-checking results** publicly document accuracy and limitations
- **Quality scoring** provides objective metrics for assessment reliability

**3. Accelerated Climate Communication**
- Faster synthesis of emerging research for policymakers
- Ability to quickly update assessments as new evidence emerges
- Multiple perspectives (USA, Europe, China) for global representation

### For Science

**1. Research Acceleration**
- **Rapid literature synthesis**: AI can process thousands of papers to identify key findings
- **Consistency checking**: Multi-model generation reveals contradictions in literature
- **Gap identification**: Comparison highlights areas needing more research

**2. Methodological Innovation**
- **Reproducible assessment pipelines**: Exact same prompts generate comparable outputs
- **Quality metrics**: Objective evaluation of synthesis quality (1-7 Likert scales)
- **Citation requirements**: V2 prompts enforce evidence-based claims with full references

**3. Enhanced Peer Review**
- AI-generated drafts provide starting points for expert refinement
- **Fact-checking frameworks** catch errors before human review
- **Cross-model validation**: Different models check each other's work

### For Policy

**1. Timely Decision Support**
- **Near-instant synthesis** of latest research for urgent policy questions
- **Scenario exploration**: Quick generation of assessment variants for different policy contexts
- **Evidence accessibility**: Direct citations link claims to peer-reviewed sources

**2. Improved Assessment Quality**
- **Objective quality metrics** (accuracy, IPCC compliance, comprehensiveness)
- **Multi-perspective analysis**: Models from different providers/nations offer diverse viewpoints
- **Consistency**: Automated generation ensures uniform style and structure

**3. Cost Efficiency**
- **Reduced assessment costs**: $5-20 vs millions for traditional IPCC assessments
- **Faster updates**: Weeks instead of years for assessment cycles
- **Resource allocation**: Frees human experts for high-value validation and refinement

---

## üéØ Transparency Principles

### 1. Open Methodology
- **All prompts publicly available** (see `prompts/`)
- **Evaluation criteria documented** (7-point Likert scales with justifications)
- **Model configurations transparent** (exact model IDs and parameters)

### 2. Rigorous Evaluation
- **Fact-checking**: Every model output evaluated for errors
- **Quality scoring**: Multi-dimensional assessment (accuracy, style, intelligence)
- **Comparative analysis**: Models ranked objectively on performance

### 3. Limitations Acknowledged
- **AI hallucinations documented**: Fact-check reports show specific errors
- **Quality variations noted**: Some models fail to meet standards
- **Human validation required**: AI is tool for acceleration, not replacement

### 4. Reproducibility
- **All code open source**
- **Exact prompts preserved**
- **Results fully documented** with statistics and examples

---

## üìä Key Results

**Best Model**: Google Gemini 2.5 Pro (perfect 7.00/7 score)
**Total Output**: 365,000+ words across all tests
**PDF Success Rate**: 100% (all formatting issues resolved)
**Working Models**: 5/7 verified
**Production Ready**: ‚úÖ Yes

See `PREMIUM_TIER_FINAL_REPORT.md` and `ULTIMATE_FINAL_REPORT.md` for complete results.

---

**Project Status**: ‚úÖ Production-Ready
**All Tests**: ‚úÖ Complete
**Documentation**: ‚úÖ Comprehensive

*Demonstrating transparent, evaluated, and reproducible AI-assisted scientific assessment generation for the benefit of global publics, science, and policy.*

---

## üë• About

### AI Lab for Book-Lovers

This project is developed by the **AI Lab for Book-Lovers**, exploring innovative applications of AI in scientific communication and knowledge synthesis.

üîó **Visit**: [codexes.xtuff.ai](https://codexes.xtuff.ai)  
üìß **Subscribe**: [AI Lab Substack](https://ailabforbooklovers.substack.com)

<iframe src="https://ailabforbooklovers.substack.com/embed" width="480" height="320" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>

### Project Lead

**Fred Zannarbor** brings 25+ years of climate change analytics experience at ERIM, CIESIN/SEDAC (Columbia University), and ISciences, combining geospatial analysis with policy support.

### Purpose

This project demonstrates:
1. **Technical capability** for AI-assisted climate assessment generation
2. **Transparent evaluation** frameworks for quality assurance
3. **Multi-model comparison** to understand AI strengths and limitations
4. **Open methodology** for community validation and improvement

**Note**: This is a **technical demonstration and feedback-seeking release**. We have demonstrated reliable text generation and evaluation capabilities. User acceptance testing and expert validation are needed next steps.

### Feedback Welcome

We seek input on:
- Quality of AI-generated climate assessment content
- Evaluation framework effectiveness
- Appropriate use cases for AI assistance
- Areas for improvement and enhancement

Please open issues or contribute via pull requests on GitHub.

---

## üìú Disclaimer

This project is **not affiliated with the IPCC** and does not represent actual IPCC assessment work. It is a demonstration of AI capabilities for scientific synthesis. All AI-generated content requires expert validation before use in policy or decision-making contexts.

